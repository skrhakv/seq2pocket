{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "965fb004",
   "metadata": {},
   "source": [
    "## Activate venv\n",
    "`source ~/cryptic-nn/src/fine-tuning/esmc-venv/bin/activate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4b732",
   "metadata": {},
   "source": [
    "# Define model\n",
    "Define the finetuning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94b665ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PREDICTION SOURCE CODE ###\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/skrhakv/cryptic-nn/src')\n",
    "import finetuning_utils\n",
    "\n",
    "torch.manual_seed(420)\n",
    "\n",
    "ESM_MODEL_NAME = 'facebook/esm2_t36_3B_UR50D'\n",
    "MAX_LENGTH = 1024\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUTPUT_SIZE = 1\n",
    "DROPOUT = 0.25\n",
    "SEQUENCE_MAX_LENGTH = MAX_LENGTH - 2\n",
    "# download: https://owncloud.cesnet.cz/index.php/s/mPo9NCWMBouwVAo/download\n",
    "MODEL_PATH = \"/home/skrhakv/cryptic-nn/final-data/trained-models/multitask-finetuned-model-with-ligysis.pt\"\n",
    "DECISION_THRESHOLD = 0.7  # Threshold to consider a point as high score\n",
    "\n",
    "\n",
    "class FinetuneESM(nn.Module):\n",
    "    def __init__(self, esm_model: str) -> None:\n",
    "        super().__init__()\n",
    "        self.llm = EsmModel.from_pretrained(esm_model)\n",
    "        self.dropout = nn.Dropout(DROPOUT)\n",
    "        self.classifier = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE)\n",
    "        self.plDDT_regressor = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE)\n",
    "        self.distance_regressor = nn.Linear(self.llm.config.hidden_size, OUTPUT_SIZE)\n",
    "\n",
    "    def forward(self, batch: dict[str, np.ndarray]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        input_ids, attention_mask = batch[\"input_ids\"], batch[\"attention_mask\"]\n",
    "        token_embeddings = self.llm(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        return (\n",
    "            self.classifier(token_embeddings),\n",
    "            self.plDDT_regressor(token_embeddings),\n",
    "            self.distance_regressor(token_embeddings),\n",
    "        )\n",
    "\n",
    "\n",
    "def compute_prediction(sequence: str, emb_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the residue-level prediction using the CryptoBench model.\n",
    "\n",
    "    Args:\n",
    "        sequence (str): Sequence of amino acids to be predicted.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The predicted scores for each residue.\n",
    "    \"\"\"\n",
    "    model = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ESM_MODEL_NAME)\n",
    "    model.eval()\n",
    "\n",
    "    sequence = str(sequence)\n",
    "    all_embeddings = []\n",
    "    final_output = []\n",
    "\n",
    "    # Process sequence in chunks of SEQUENCE_MAX_LENGTH\n",
    "    for i in range(0, len(sequence), SEQUENCE_MAX_LENGTH):\n",
    "        processed_sequence = sequence[i : i + SEQUENCE_MAX_LENGTH]\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            processed_sequence, max_length=MAX_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        tokenized = {k: v.to(DEVICE) for k, v in tokenized.items()}\n",
    "\n",
    "        # embeddings\n",
    "        with torch.no_grad():\n",
    "            llm_output = model.llm(input_ids=tokenized[\"input_ids\"], attention_mask=tokenized[\"attention_mask\"])\n",
    "            embeddings = llm_output.last_hidden_state  # shape: (1, seq_len, hidden_dim)\n",
    "\n",
    "        embeddings_np = embeddings.squeeze(0).detach().cpu().numpy()\n",
    "        mask = tokenized[\"attention_mask\"].squeeze(0).detach().cpu().numpy().astype(bool)\n",
    "        embeddings_np = embeddings_np[mask][1:-1]  # exclude [CLS], [SEP]\n",
    "        all_embeddings.append(embeddings_np)\n",
    "\n",
    "        # prediction\n",
    "        with torch.no_grad():\n",
    "            output, _, _ = model(tokenized)\n",
    "\n",
    "        output = output.squeeze(0)\n",
    "        mask = tokenized[\"attention_mask\"].squeeze(0).bool()\n",
    "        output = output[mask][1:-1]  # exclude [CLS], [SEP]\n",
    "\n",
    "        probabilities = torch.sigmoid(output).detach().cpu().numpy()\n",
    "        final_output.extend(probabilities)\n",
    "\n",
    "    # save the concatenated embeddings for the entire sequence\n",
    "    final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    save_path = os.path.join(emb_path)\n",
    "    np.save(save_path, final_embeddings)\n",
    "\n",
    "    return np.array(final_output).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b5809",
   "metadata": {},
   "source": [
    "# TODO: DATA LEAKAGE!\n",
    "retrain the 650M model on the LIGYSIS without leaking the information to CryptoBench subset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42ed0b8",
   "metadata": {},
   "source": [
    "## COPYRIGHT NOTICE\n",
    "The following code cell was taken from the following repository: https://github.com/luk27official/cryptoshow-benchmark/\n",
    "\n",
    "For further info see the LICENSE file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264e69e7",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cd397",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "Define functions for clustering and smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f7fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTED_RESIDUE_RADIUS_DISTANCE_THRESHOLD = 10 # in Angstroms; for each predicted binding residue, we consider all residues within this distance as candidate residues for inclusion into the binding site\n",
    "CANDIDATE_RESIDUE_SURROUNDING_RADIUS_THRESHOLD = 15 # in Angstroms; for each candidate residue that is considered for inclusion into the binding site, we define the surrounding binding site respresentation as \n",
    "                                                    # the mean of embeddings of all predicted binding residues within this distance\n",
    "\n",
    "def process_single_sequence(structure_name: str, chain_id: str, binding_residues_indices: np.ndarray, embedding_path: str, distance_matrix: np.ndarray):\n",
    "    id = structure_name.lower() + chain_id\n",
    "    if not os.path.exists(embedding_path):\n",
    "        raise FileNotFoundError(f'Embedding file for {id} not found in {embedding_path}')\n",
    "    \n",
    "    embedding = np.load(embedding_path)\n",
    "\n",
    "    Xs = []\n",
    "    idx = []\n",
    "    \n",
    "    candidate_residues_indices = set()\n",
    "    \n",
    "    for residue_idx in binding_residues_indices:\n",
    "        close_residues_indices = np.where(distance_matrix[residue_idx] < PREDICTED_RESIDUE_RADIUS_DISTANCE_THRESHOLD)[0]\n",
    "        close_binding_residues_indices = np.intersect1d(close_residues_indices, binding_residues_indices)\n",
    "\n",
    "        candidate_residues_indices.update(set(list(close_residues_indices)) - set(list(binding_residues_indices)))\n",
    "\n",
    "    for residue_idx in candidate_residues_indices:\n",
    "        close_residues_indices = np.where(distance_matrix[residue_idx] < CANDIDATE_RESIDUE_SURROUNDING_RADIUS_THRESHOLD)[0]\n",
    "        close_binding_residues_indices = np.intersect1d(close_residues_indices, binding_residues_indices)\n",
    "\n",
    "        concatenated_embedding = np.concatenate((embedding[residue_idx], np.mean(embedding[close_binding_residues_indices], axis=0)))\n",
    "        Xs.append(concatenated_embedding)\n",
    "        idx.append(residue_idx)\n",
    "        \n",
    "    return np.array(Xs), np.array(idx)\n",
    "\n",
    "def predict_single_sequence(Xs, idx, model_3):\n",
    "    Xs = torch.tensor(Xs, dtype=torch.float32).to(DEVICE)\n",
    "    idx = torch.tensor(idx, dtype=torch.int64).to(DEVICE)\n",
    "\n",
    "    test_logits = model_3(Xs).squeeze()\n",
    "    test_pred = torch.sigmoid(test_logits)\n",
    "\n",
    "    return {'predictions': test_pred.detach().cpu().numpy(), 'indices': idx.detach().cpu().numpy()}\n",
    "\n",
    "def compute_distance_matrix(coordinates: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the pairwise distance matrix for a given set of coordinates.\n",
    "\n",
    "    Args:\n",
    "        coordinates (np.ndarray): A 2D array of shape (N, 3), where N is the number of points,\n",
    "                                   and each row represents the (x, y, z) coordinates of a point.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D array of shape (N, N), where the element at [i, j] represents the Euclidean\n",
    "                    distance between the i-th and j-th points.\n",
    "    \"\"\"\n",
    "    coordinates = np.array(coordinates)\n",
    "    distance_matrix = np.linalg.norm(coordinates[:, np.newaxis] - coordinates[np.newaxis, :], axis=-1)\n",
    "    return distance_matrix\n",
    "\n",
    "\n",
    "SMOOTHING_DECISION_THRESHOLD = 0.4 # see src/C-optimize-smoother/classifier-for-cryptoshow.ipynb\n",
    "DROPOUT = 0.5\n",
    "LAYER_WIDTH = 2048\n",
    "ESM2_DIM = 2560\n",
    "INPUT_DIM  = ESM2_DIM * 2\n",
    "\n",
    "class CryptoBenchClassifier(nn.Module):\n",
    "    def __init__(self, dim=LAYER_WIDTH, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=INPUT_DIM, out_features=dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer_2 = nn.Linear(in_features=dim, out_features=dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer_3 = nn.Linear(in_features=dim, out_features=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      # Intersperse the ReLU activation function between layers\n",
    "       return self.layer_3(self.dropout2(self.relu(self.layer_2(self.dropout1(self.relu(self.layer_1(x)))))))\n",
    "\n",
    "def compute_clusters(\n",
    "    points: list[list[float]],\n",
    "    prediction_scores: list[float],\n",
    "):\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    \"\"\"\n",
    "    Compute clusters based on the given points and prediction scores.\n",
    "\n",
    "    Args:\n",
    "        points (list[list[float]]): A list of points, where each point is a list of 3 coordinates [x, y, z].\n",
    "        prediction_scores (list[float]): A list of prediction scores corresponding to each point.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of cluster labels for each point. Points with no cluster are labeled as -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_scores = prediction_scores.reshape(-1, 1)\n",
    "    stacked = np.hstack((points, prediction_scores))  # Combine coordinates with scores\n",
    "\n",
    "    high_score_mask = stacked[:, 3] > DECISION_THRESHOLD\n",
    "    high_score_points = stacked[high_score_mask][:, :3]  # Extract only (x, y, z) coordinates\n",
    "\n",
    "    EPS = 5.0  # Max distance for neighbors\n",
    "    MIN_SAMPLES = 3  # Min points to form a cluster\n",
    "\n",
    "    # No pockets can be formed if there are not enough high score points.\n",
    "    if len(high_score_points) < MIN_SAMPLES:\n",
    "        return -1 * np.ones(len(points), dtype=int)\n",
    "\n",
    "    dbscan = DBSCAN(eps=EPS, min_samples=MIN_SAMPLES)\n",
    "    labels = dbscan.fit_predict(high_score_points)\n",
    "\n",
    "    # Initialize all labels to -1\n",
    "    all_labels = -1 * np.ones(len(points), dtype=int)\n",
    "    # Assign cluster labels to high score points\n",
    "    all_labels[high_score_mask] = labels\n",
    "    labels = all_labels\n",
    "\n",
    "    return labels\n",
    "\n",
    "import csv\n",
    "# the LIGYSIS-ed CryptoBench test set\n",
    "TEST_DATA_PATH = '/home/skrhakv/cryptoshow-analysis/data/A-cluster-ligysis-data/clustered-binding-sites.txt'\n",
    "\n",
    "def read_test_binding_residues() -> set[int]:\n",
    "    cryptic_binding_residues = {}\n",
    "    noncryptic_binding_residues = {}\n",
    "    sequences = {}\n",
    "\n",
    "    with open(TEST_DATA_PATH, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        for row in reader:\n",
    "            protein_id = row[0]\n",
    "            if row[3] == '':\n",
    "                continue\n",
    "            binding_residue_indices = [int(i[1:])for i in row[3].split(' ')]\n",
    "            sequence = row[4]\n",
    "            if row[2] == 'CRYPTIC':\n",
    "                if protein_id not in cryptic_binding_residues:\n",
    "                    cryptic_binding_residues[protein_id] = []\n",
    "                cryptic_binding_residues[protein_id].append(binding_residue_indices)\n",
    "            else:\n",
    "                if protein_id not in noncryptic_binding_residues:\n",
    "                    noncryptic_binding_residues[protein_id] = []\n",
    "                noncryptic_binding_residues[protein_id].append(binding_residue_indices)\n",
    "            sequences[protein_id] = sequence\n",
    "\n",
    "    return cryptic_binding_residues, noncryptic_binding_residues, sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a786578",
   "metadata": {},
   "source": [
    "### Load\n",
    "Loading the smoothing classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "486ebd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOOTHING_MODEL_PATH = '/home/skrhakv/cryptoshow-analysis/data/C-optimize-smoother/smoother.pt'\n",
    "smoothing_model = torch.load(SMOOTHING_MODEL_PATH, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325823c",
   "metadata": {},
   "source": [
    "### Compute\n",
    "Predict the sequence, cluster the predictions and smoothen it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b87b1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "COORDINATES_DIR = '/home/skrhakv/cryptoshow-analysis/data/A-cluster-ligysis-data/coordinates'\n",
    "EMBEDDINGS_DIR = '/home/skrhakv/cryptoshow-analysis/data/B-evaluate-cryptoshow/embeddings'\n",
    "cryptic_binding_residues, noncryptic_binding_residues, sequences = read_test_binding_residues()\n",
    "\n",
    "predicted_binding_sites = {}\n",
    "for protein_id in sequences.keys():\n",
    "    pdb_id = protein_id[:4]\n",
    "    chain_id = protein_id[4:]\n",
    "    sequence = sequences[protein_id]\n",
    "    embeddings_path = f'{EMBEDDINGS_DIR}/{pdb_id}{chain_id}.npy'\n",
    "    coordinates_path = f'{COORDINATES_DIR}/{pdb_id}{chain_id}.npy'\n",
    "    \n",
    "    # finetuned prediction\n",
    "    prediction = compute_prediction(\n",
    "        sequence,\n",
    "        embeddings_path\n",
    "    )\n",
    "    \n",
    "    # cluster predicted pockets\n",
    "    coordinates = np.load(coordinates_path)\n",
    "    clusters = compute_clusters(\n",
    "        coordinates,\n",
    "        prediction\n",
    "    )\n",
    "\n",
    "    # load distance matrix\n",
    "    distance_matrix = compute_distance_matrix(coordinates)\n",
    "\n",
    "    # enhance predicted pockets using the smoothing model\n",
    "    predicted_binding_sites[protein_id] = []\n",
    "    for cluster_label in np.unique(clusters):\n",
    "        if cluster_label == -1:\n",
    "            continue\n",
    "        cluster_residue_indices = np.where(clusters == cluster_label)[0]\n",
    "        embeddings, indices = process_single_sequence(pdb_id, chain_id, cluster_residue_indices, embeddings_path, distance_matrix) \n",
    "        prediction = predict_single_sequence(embeddings, indices, smoothing_model)\n",
    "        enhanced_residue_indices = np.concatenate((indices[prediction['predictions'] > SMOOTHING_DECISION_THRESHOLD], cluster_residue_indices))\n",
    "        predicted_binding_sites[protein_id].append(enhanced_residue_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d06c78",
   "metadata": {},
   "source": [
    "### Residue-level stats\n",
    "Take the predictions from the previous steps and calculate residue-level metrics: F1, MCC, ACC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3e96b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9285945654129926 0.455878939471634 0.9278459044017294\n"
     ]
    }
   ],
   "source": [
    "actual = []\n",
    "predicted = []\n",
    "\n",
    "from sklearn import metrics\n",
    "for protein_id in cryptic_binding_residues.keys():\n",
    "    this_actual = np.zeros(len(sequences[protein_id]), dtype=int)\n",
    "    for binding_site in cryptic_binding_residues[protein_id]:\n",
    "        this_actual[binding_site] = 1\n",
    "    this_predicted = np.zeros(len(sequences[protein_id]), dtype=int)\n",
    "    for binding_site in predicted_binding_sites[protein_id]:\n",
    "        this_predicted[binding_site] = 1\n",
    "    predicted.append(this_predicted)\n",
    "    actual.append(this_actual)\n",
    "\n",
    "labels = np.concatenate(actual)\n",
    "predictions = np.concatenate(predicted)\n",
    "\n",
    "acc = metrics.accuracy_score(labels, predictions)\n",
    "mcc = metrics.matthews_corrcoef(labels, predictions)\n",
    "f1 = metrics.f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "print(acc, mcc, f1)\n",
    "\n",
    "# ACC                MCC                 F1\n",
    "# 0.9285225841281267 0.45744821711536815 0.9279191632095326  - with clustering and smoothing\n",
    "# 0.9319596904804751 0.4961393895010758  0.9322347096744608  - without clustering and smoothing\n",
    "# 0.9229980205146662 0.48906673975928316 0.9269531602777475  - with clustering without smoothing\n",
    "# 0.8786395537160339 0.4482652961836764 0.8980359042250458   - old version of classifier with clustering without smoothing\n",
    "# try old version of classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea942c",
   "metadata": {},
   "source": [
    "### No clustering/smoothing\n",
    "Only predict using the sequence, without any clustering or smoothing. However, use the same evaluation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca468339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9319596904804751 0.4961393895010758 0.9322347096744608\n"
     ]
    }
   ],
   "source": [
    "no_clustering_predicted = []\n",
    "for protein_id in sequences.keys():\n",
    "    pdb_id = protein_id[:4]\n",
    "    chain_id = protein_id[4:]\n",
    "    sequence = sequences[protein_id]\n",
    "    embeddings_path = f'/home/skrhakv/cryptoshow-analysis/data/B-evaluate-cryptoshow/embeddings/{pdb_id}{chain_id}.npy'\n",
    "    coordinates_path = f'/home/skrhakv/cryptoshow-analysis/data/A-cluster-ligysis-data/coordinates/{pdb_id}{chain_id}.npy'\n",
    "    \n",
    "    prediction = compute_prediction(\n",
    "        sequence,\n",
    "        embeddings_path\n",
    "    )\n",
    "    this_predicted = (prediction > DECISION_THRESHOLD).astype(int)\n",
    "    no_clustering_predicted.append(this_predicted)\n",
    "predictions = np.concatenate(no_clustering_predicted)\n",
    "\n",
    "acc = metrics.accuracy_score(labels, predictions)\n",
    "mcc = metrics.matthews_corrcoef(labels, predictions)\n",
    "f1 = metrics.f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "print(acc, mcc, f1)\n",
    "# the results are different than in the paper (Hidden in protein sequences: Predicting cryptic binding sites), because the test set is slightly different - the binding residues were completed by LIGYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f019fb",
   "metadata": {},
   "source": [
    "### Pocket-level stats\n",
    "Compute DCC and DCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffff16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e55ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_center_distance(actual_points: np.ndarray, predicted_points: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the distance from the predicted pocket center to the expected pocket center.\n",
    "    Args:\n",
    "        actual_points (np.ndarray): Array of shape (N, 3) containing the coordinates of the actual binding residues.\n",
    "        predicted_points (np.ndarray): Array of shape (M, 3) containing the coordinates of the predicted binding residues.\n",
    "    Returns:\n",
    "        float: The DCC value, which is the distance from the predicted pocket center to the ligand center.\n",
    "    \"\"\"\n",
    "    expected_coords = predicted_points.mean(axis=0)\n",
    "    actual_coords = actual_points.mean(axis=0)\n",
    "\n",
    "    dist = np.linalg.norm(expected_coords - actual_coords)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9118c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "DCCs = []\n",
    "for protein_id in cryptic_binding_residues.keys():\n",
    "      if len(predicted_binding_sites[protein_id]) == 0 or len(cryptic_binding_residues[protein_id]) == 0:\n",
    "            continue\n",
    "      coordinates = np.load(f'{COORDINATES_DIR}/{protein_id}.npy')\n",
    "\n",
    "      # loop over each cryptic binding site\n",
    "      for actual_cryptic_binding_residue_indices in cryptic_binding_residues[protein_id]:\n",
    "            dcc = float('inf')\n",
    "            actual_coordinates = coordinates[actual_cryptic_binding_residue_indices]\n",
    "            # loop over each predicted binding site and select the one with the lowest DCC\n",
    "            for predicted_cryptic_binding_residue_indices in predicted_binding_sites[protein_id]:\n",
    "                dcc = min(dcc, compute_center_distance(actual_coordinates, coordinates[predicted_cryptic_binding_residue_indices]))\n",
    "            DCCs.append(dcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8d6ad99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.417748"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(DCCs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
