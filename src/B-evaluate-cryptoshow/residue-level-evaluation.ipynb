{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4733439d",
   "metadata": {},
   "source": [
    "# Intro\n",
    "Code copied from the `cryptic-finetuning` repository history to replicate the results that were calculated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b634bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import sklearn.metrics as metrics\n",
    "import functools\n",
    "import sys\n",
    "sys.path.append('/home/skrhakv/cryptic-nn/src')\n",
    "import finetuning_utils\n",
    "import baseline_utils\n",
    "\n",
    "torch.manual_seed(420)\n",
    "OUTPUT_PATH = \"/home/skrhakv/cryptic-nn/final-data/trained-models/multitask-finetuned-model-with-ligysis.pt\"\n",
    "ESM_MODEL_NAME = 'facebook/esm2_t36_3B_UR50D'\n",
    "MAX_LENGTH = 1024\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DECISION_THRESHOLD = 0.7\n",
    "loaded_model = torch.load(OUTPUT_PATH, weights_only=False).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ESM_MODEL_NAME)\n",
    "partial_collate_fn = functools.partial(finetuning_utils.collate_fn, tokenizer=tokenizer)\n",
    "val_dataset = finetuning_utils.process_sequence_dataset(\n",
    "    '/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', \n",
    "    tokenizer,\n",
    "    load_ids=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6076a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CryptoBenchClassifier(\n",
       "  (layer_1): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (layer_2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (layer_3): Linear(in_features=2048, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMOOTHING_MODEL_PATH = '/home/skrhakv/cryptoshow-analysis/data/C-optimize-smoother/smoother.pt'\n",
    "\n",
    "SMOOTHING_DECISION_THRESHOLD = 0.4 # see src/C-optimize-smoother/classifier-for-cryptoshow.ipynb\n",
    "DROPOUT = 0.5\n",
    "LAYER_WIDTH = 2048\n",
    "ESM2_DIM = 2560\n",
    "INPUT_DIM  = ESM2_DIM * 2\n",
    "\n",
    "class CryptoBenchClassifier(nn.Module):\n",
    "    def __init__(self, dim=LAYER_WIDTH, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=INPUT_DIM, out_features=dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer_2 = nn.Linear(in_features=dim, out_features=dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.layer_3 = nn.Linear(in_features=dim, out_features=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "      # Intersperse the ReLU activation function between layers\n",
    "       return self.layer_3(self.dropout2(self.relu(self.layer_2(self.dropout1(self.relu(self.layer_1(x)))))))\n",
    "\n",
    "\n",
    "smoothing_model = torch.load(SMOOTHING_MODEL_PATH, weights_only=False).to(DEVICE)\n",
    "smoothing_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5070d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.36% |  MCC: 0.4604, F1: 0.9318\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = f'/home/skrhakv/cryptic-nn/data/cryptobench'\n",
    "ESM_EMBEDDINGS_PATH = f'{DATA_PATH}/embeddings'\n",
    "DISTANCE_MATRICES_PATH = f'{DATA_PATH}/distance-matrices'\n",
    "POSITIVE_DISTANCE_THRESHOLD = 15\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_test_logits = []\n",
    "    all_test_pred = []\n",
    "    all_y_test = []\n",
    "    this_test_losses = []\n",
    "\n",
    "    for batch in val_dataset:\n",
    "        \n",
    "        protein_id = batch['ids'][0]\n",
    "        del batch['ids']\n",
    "        batch = finetuning_utils.collate_fn([batch], tokenizer=tokenizer)\n",
    "        output1, _, _ = loaded_model(batch)\n",
    "\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "        flattened_labels = labels.flatten()\n",
    "        y_test = flattened_labels[flattened_labels != -100]\n",
    "        logits = output1.flatten()[flattened_labels != -100]\n",
    "\n",
    "        test_pred = (torch.sigmoid(logits)>DECISION_THRESHOLD).float()\n",
    "        test_pred_copy = test_pred.clone().detach().cpu().numpy()\n",
    "\n",
    "        # let's use the smoothing model here:\n",
    "        # first, get the distance matrix\n",
    "        distance_matrix = np.load(f'{DISTANCE_MATRICES_PATH}/{protein_id}.npy')\n",
    "        \n",
    "        assert distance_matrix.shape[0] == distance_matrix.shape[1]\n",
    "        assert distance_matrix.shape[0] == test_pred_copy.shape[0]\n",
    "        \n",
    "        X_test = np.load(f'{DATA_PATH}/embeddings/{protein_id}.npy')\n",
    "        assert X_test.shape[0] == distance_matrix.shape[0]\n",
    "\n",
    "        # loop over the residues that are not binding and are potential candidates for smoothing\n",
    "        for residue_idx in torch.where(test_pred == 0.0)[0]:\n",
    "            # get the embedding of the residue\n",
    "            current_residue_embedding = X_test[residue_idx]\n",
    "\n",
    "            # get the close binding residues\n",
    "            close_residues_indices = np.where(distance_matrix[residue_idx] < POSITIVE_DISTANCE_THRESHOLD)[0]\n",
    "            close_binding_residues_indices = np.intersect1d(close_residues_indices, torch.where(test_pred == 1.0)[0].cpu().numpy())\n",
    "\n",
    "            # create embedding \n",
    "            if len(close_binding_residues_indices) == 0:\n",
    "                # no close binding residues - skip this residue\n",
    "                continue\n",
    "            elif len(close_binding_residues_indices) == 1:\n",
    "                surrounding_embedding = X_test[close_binding_residues_indices].reshape(-1)\n",
    "            else:\n",
    "                # get the mean of the close binding residues\n",
    "                surrounding_embedding = np.mean(X_test[close_binding_residues_indices], axis=0).reshape(-1)\n",
    "\n",
    "            concatenated_embedding = torch.tensor(np.concatenate((current_residue_embedding, surrounding_embedding), axis=0), dtype=torch.float32).to(DEVICE)\n",
    "            \n",
    "            # get the prediction\n",
    "            test_logits = smoothing_model(concatenated_embedding).squeeze()\n",
    "            result = (torch.sigmoid(test_logits)>SMOOTHING_DECISION_THRESHOLD).float()\n",
    "            if result == 1:\n",
    "                # set the residue as binding\n",
    "                test_pred_copy[residue_idx] = 1\n",
    "        \n",
    "        all_test_logits.append(logits.cpu().detach().numpy())\n",
    "        all_y_test.append(y_test.cpu().detach().numpy())\n",
    "        all_test_pred.append(test_pred_copy)\n",
    "        assert len(y_test) == len(test_pred_copy)        \n",
    "    test_logits = torch.tensor(np.concatenate(all_test_logits, axis=0), dtype=torch.float32).to(DEVICE)\n",
    "    test_pred = torch.tensor(np.concatenate(all_test_pred, axis=0), dtype=torch.float32).to(DEVICE)\n",
    "    y_test = torch.tensor(np.concatenate(all_y_test, axis=0), dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    # compute metrics on test dataset\n",
    "    test_acc = baseline_utils.accuracy_fn(y_true=y_test,\n",
    "                            y_pred=test_pred)\n",
    "\n",
    "    mcc = metrics.matthews_corrcoef(y_test.cpu().numpy(), test_pred.cpu().numpy())\n",
    "\n",
    "    f1 = metrics.f1_score(y_test.cpu().numpy(), test_pred.cpu().numpy(), average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {test_acc:.2f}% |  MCC: {mcc:.4f}, F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea723e8",
   "metadata": {},
   "source": [
    "## Compare it with model without smoothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9294a026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.51% | AUC: 0.8935, MCC: 0.4720, F1: 0.9389, AUPRC: 0.4752\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "partial_collate_fn = functools.partial(finetuning_utils.collate_fn, tokenizer=tokenizer)\n",
    "val_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.num_rows, collate_fn=partial_collate_fn)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        output1, _, _ = loaded_model(batch)\n",
    "\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output1.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        predictions = (torch.sigmoid(cbs_logits)>DECISION_THRESHOLD).float()\n",
    "\n",
    "        # compute metrics on test dataset\n",
    "        test_acc = baseline_utils.accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                y_pred=predictions)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "\n",
    "        f1 = metrics.f1_score(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "\n",
    "print(f\"Accuracy: {test_acc:.2f}% | AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
