{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b392db",
   "metadata": {},
   "source": [
    "# external libraries\n",
    "We are using libraries `baseline_utils` and `finetuning_utils` from this repository: https://github.com/skrhakv/cryptic-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8366ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys \n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import functools\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "\n",
    "sys.path.append('/home/skrhakv/cryptic-nn/src')\n",
    "import baseline_utils\n",
    "import finetuning_utils\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(42)\n",
    "\n",
    "MODEL_NAME = 'facebook/esm2_t36_3B_UR50D'\n",
    "DATASET = 'cryptobench'\n",
    "DATA_PATH = f'/home/skrhakv/cryptic-nn/data/{DATASET}'\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c201d1",
   "metadata": {},
   "source": [
    "## Finetuning ESM2 model\n",
    "Let's finetune the whole ESM2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f3fda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c509661be584977aea825712848e532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    102\u001b[39m valid_flattened_labels = labels.flatten()[flattened_labels != -\u001b[32m100\u001b[39m]\n\u001b[32m    104\u001b[39m loss =  loss_fn(cbs_logits, valid_flattened_labels)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m loss.backward()\n\u001b[32m    108\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/_compile.py:32\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     30\u001b[39m     fn.__dynamo_disable = disable_fn\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    741\u001b[39m prior_skip_guard_eval_unsafe = set_skip_guard_eval_unsafe(\n\u001b[32m    742\u001b[39m     _is_skip_guard_eval_unsafe_stance()\n\u001b[32m    743\u001b[39m )\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    747\u001b[39m     _maybe_set_eval_frame(prior)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cryptic-nn/src/fine-tuning/esmc-venv/lib/python3.12/site-packages/torch/optim/optimizer.py:973\u001b[39m, in \u001b[36mOptimizer.zero_grad\u001b[39m\u001b[34m(self, set_to_none)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    972\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    974\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[32m    975\u001b[39m                 p.grad = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "finetuned_model = finetuning_utils.FinetunedEsmModel(MODEL_NAME).half().to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptoshow-analysis/data/E-regular-binding-site-predictor/scPDB_enhanced_binding_sites_translated.csv', tokenizer)\n",
    "val_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptoshow-analysis/data/E-regular-binding-site-predictor/ligysis_without_unobserved.csv', tokenizer) \n",
    "\n",
    "partial_collate_fn = functools.partial(finetuning_utils.collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=int(val_dataset.num_rows / 20), collate_fn=partial_collate_fn)\n",
    "\n",
    "optimizer = bnb.optim.AdamW8bit(finetuned_model.parameters(), lr=0.0001, eps=1e-4) \n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "# precomputed class weights\n",
    "for batch in train_dataloader:\n",
    "    labels = batch['labels']\n",
    "\n",
    "class_labels = labels.cpu().numpy().reshape(-1)[labels.cpu().numpy().reshape(-1) >= 0]\n",
    "weights = baseline_utils.compute_class_weights(class_labels)\n",
    "class_weights = torch.tensor(weights, device=device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "del labels, class_labels\n",
    "\n",
    "for name, param in finetuned_model.named_parameters():\n",
    "     if name.startswith('llm'): \n",
    "        param.requires_grad = False\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    if epoch > 1:\n",
    "        for name, param in finetuned_model.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    finetuned_model.eval()\n",
    "\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            output = finetuned_model(batch)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "    \n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            cbs_logits = output.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            logits_list.append(cbs_logits.cpu().float().detach().numpy())\n",
    "            labels_list.append(valid_flattened_labels.cpu().float().detach().numpy())\n",
    "\n",
    "            del labels, cbs_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # TODO: is it going to fail on memory or not when using LYGISYS?\n",
    "        cbs_logits = torch.tensor(np.concatenate(logits_list)).to(device)\n",
    "        valid_flattened_labels = torch.tensor(np.concatenate(labels_list)).to(device)\n",
    "\n",
    "        predictions = torch.round(torch.sigmoid(cbs_logits)) # (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "        cbs_test_loss =  loss_fn(cbs_logits, valid_flattened_labels)\n",
    "\n",
    "        test_loss = cbs_test_loss\n",
    "\n",
    "        test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "\n",
    "        # compute metrics on test dataset\n",
    "        test_acc = baseline_utils.accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                y_pred=predictions)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "\n",
    "        f1 = metrics.f1_score(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "\n",
    "    \n",
    "    finetuned_model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        output = finetuned_model(batch)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss =  loss_fn(cbs_logits, valid_flattened_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, output, cbs_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, sum: {sum(predictions.to(dtype=torch.int))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e3269",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/home/skrhakv/cryptoshow-analysis/data/E-regular-binding-site-predictor/model-enhanced-scPDB.pt'\n",
    "torch.save(finetuned_model, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a88195",
   "metadata": {},
   "source": [
    "# Try one extra epoch:\n",
    "\n",
    "(this didn't work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db5782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.07614, Accuracy: 92.78% | Test loss: 0.49911, AUC: 0.8205, MCC: 0.4211, F1: 0.9300, AUPRC: 0.4373, sum: 87069\n",
      "Epoch: 0 | Loss: 0.07614, Accuracy: 91.79% | Test loss: 0.53121, AUC: 0.8252, MCC: 0.4135, F1: 0.9241, AUPRC: 0.4542, sum: 106653\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = OUTPUT_PATH # f'/home/skrhakv/cryptoshow-analysis/src/E-regular-binding-site-predictor/model.pt'\n",
    "finetuned_model = torch.load(OUTPUT_PATH, weights_only=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# train_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/train.txt', tokenizer)\n",
    "# val_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptic-nn/data/cryptobench/test.txt', tokenizer)\n",
    "\n",
    "train_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/Near-Hit-Scoring/data/input/scPDB_90_SI.csv', tokenizer)\n",
    "val_dataset = finetuning_utils.process_sequence_dataset('/home/skrhakv/cryptic-nn/data/ligysis/train.txt', tokenizer) # it is called train because in other instance it was used for training but here we can use it for validation without problems\n",
    "\n",
    "partial_collate_fn = functools.partial(finetuning_utils.collate_fn, tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=partial_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=int(val_dataset.num_rows / 20), collate_fn=partial_collate_fn)\n",
    "\n",
    "optimizer = bnb.optim.AdamW8bit(finetuned_model.parameters(), lr=0.0001, eps=1e-4) \n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "# precomputed class weights\n",
    "class_weights = torch.tensor([0.5590, 4.7378], device=device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "\n",
    "test_losses = []\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    finetuned_model.eval()\n",
    "\n",
    "    # VALIDATION LOOP\n",
    "    with torch.no_grad():\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        for batch in val_dataloader:\n",
    "            output = finetuned_model(batch)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "    \n",
    "            flattened_labels = labels.flatten()\n",
    "\n",
    "            cbs_logits = output.flatten()[flattened_labels != -100]\n",
    "            valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "            logits_list.append(cbs_logits.cpu().float().detach().numpy())\n",
    "            labels_list.append(valid_flattened_labels.cpu().float().detach().numpy())\n",
    "\n",
    "            del labels, cbs_logits, valid_flattened_labels, flattened_labels\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # TODO: is it going to fail on memory or not when using LYGISYS?\n",
    "        cbs_logits = torch.tensor(np.concatenate(logits_list)).to(device)\n",
    "        valid_flattened_labels = torch.tensor(np.concatenate(labels_list)).to(device)\n",
    "\n",
    "        predictions = torch.round(torch.sigmoid(cbs_logits)) # (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "        cbs_test_loss =  loss_fn(cbs_logits, valid_flattened_labels)\n",
    "\n",
    "        test_loss = cbs_test_loss\n",
    "\n",
    "        test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "\n",
    "        # compute metrics on test dataset\n",
    "        test_acc = baseline_utils.accuracy_fn(y_true=valid_flattened_labels,\n",
    "                                y_pred=predictions)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "        mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "\n",
    "        f1 = metrics.f1_score(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "        auprc = metrics.auc(recall, precision)\n",
    "\n",
    "    \n",
    "    finetuned_model.train()\n",
    "\n",
    "    batch_losses = []\n",
    "\n",
    "    # TRAIN\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        output = finetuned_model(batch)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        loss =  loss_fn(cbs_logits, valid_flattened_labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.cpu().float().detach().numpy())\n",
    "        \n",
    "        del labels, output, cbs_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_losses.append(sum(batch_losses) / len(batch_losses))\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, sum: {sum(predictions.to(dtype=torch.int))}\")\n",
    "\n",
    "    finetuned_model.eval()\n",
    "\n",
    "# VALIDATION LOOP\n",
    "with torch.no_grad():\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        output = finetuned_model(batch)\n",
    "\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        flattened_labels = labels.flatten()\n",
    "\n",
    "        cbs_logits = output.flatten()[flattened_labels != -100]\n",
    "        valid_flattened_labels = labels.flatten()[flattened_labels != -100]\n",
    "\n",
    "        logits_list.append(cbs_logits.cpu().float().detach().numpy())\n",
    "        labels_list.append(valid_flattened_labels.cpu().float().detach().numpy())\n",
    "\n",
    "        del labels, cbs_logits, valid_flattened_labels, flattened_labels\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # TODO: is it going to fail on memory or not when using LYGISYS?\n",
    "    cbs_logits = torch.tensor(np.concatenate(logits_list)).to(device)\n",
    "    valid_flattened_labels = torch.tensor(np.concatenate(labels_list)).to(device)\n",
    "\n",
    "    predictions = torch.round(torch.sigmoid(cbs_logits)) # (probabilities>0.95).float() # torch.round(torch.sigmoid(valid_flattened_cbs_logits))\n",
    "\n",
    "    cbs_test_loss =  loss_fn(cbs_logits, valid_flattened_labels)\n",
    "\n",
    "    test_loss = cbs_test_loss\n",
    "\n",
    "    test_losses.append(test_loss.cpu().float().detach().numpy())\n",
    "\n",
    "    # compute metrics on test dataset\n",
    "    test_acc = baseline_utils.accuracy_fn(y_true=valid_flattened_labels,\n",
    "                            y_pred=predictions)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    mcc = metrics.matthews_corrcoef(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy())\n",
    "\n",
    "    f1 = metrics.f1_score(valid_flattened_labels.cpu().float().numpy(), predictions.cpu().float().numpy(), average='weighted')\n",
    "\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(valid_flattened_labels.cpu().float().numpy(), torch.sigmoid(cbs_logits).cpu().float().numpy())\n",
    "    auprc = metrics.auc(recall, precision)\n",
    "print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {test_acc:.2f}% | Test loss: {test_loss:.5f}, AUC: {roc_auc:.4f}, MCC: {mcc:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}, sum: {sum(predictions.to(dtype=torch.int))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
