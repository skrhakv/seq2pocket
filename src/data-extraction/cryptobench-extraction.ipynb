{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7212f35d",
   "metadata": {},
   "source": [
    "# Extract Uniprot Ids\n",
    "Use the IDs to scan the LIGYSIS-web database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e537f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATA_PATH = '/home/vit/Projects/cryptoshow-analysis/data/data-extraction'\n",
    "DATASET_PATH = F'{DATA_PATH}/cryptobench-dataset/folds/test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef40c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_PATH, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "uniprot_ids = [entry[0]['uniprot_id'] for key, entry in data.items() if '-' not in entry[0]['uniprot_id']]\n",
    "\n",
    "with open(f'{DATA_PATH}/uniprot_ids.txt', 'w') as f_out:\n",
    "    for uid in uniprot_ids:\n",
    "        f_out.write(f\"{uid}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff0da66",
   "metadata": {},
   "source": [
    "# Check Uniprot availability in LIGYSIS-web\n",
    "Check which Uniprot IDs are available in UniProt. [Source](github.com/bartongroup/LIGYSIS-web/blob/master/static/data/LIGYSIS_protein_names_dict_RF3.pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98372f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "DATASET_PATH = f'{DATA_PATH}/cryptobench-dataset/folds/test.json'\n",
    "\n",
    "with open(DATASET_PATH, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "uniprot_ids = [entry[0]['uniprot_id'] for key, entry in data.items() if '-' not in entry[0]['uniprot_id']]\n",
    "\n",
    "with open(f'{DATA_PATH}/LIGYSIS-files/LIGYSIS_protein_names_dict_RF3.pkl', 'rb') as f:\n",
    "    pkl_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04448bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_pkl_ids = set(pkl_data.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa414a47",
   "metadata": {},
   "source": [
    "## Which UniProt IDs were missing in the LIGYSIS-web\n",
    "214/222 Uniprot IDs are available in LIGYSIS-web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0649f8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D2B3F1\n",
      "Q9H0M0\n",
      "Q8NB16\n",
      "A9WIU3\n",
      "A0A3E2YLT4\n",
      "P50286\n",
      "Q8DQ84\n",
      "Q86W50\n"
     ]
    }
   ],
   "source": [
    "for uniprot_id in uniprot_ids:\n",
    "    if uniprot_id not in set_pkl_ids:\n",
    "        print(uniprot_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254bfdc",
   "metadata": {},
   "source": [
    "# Load LIGYSIS data\n",
    "Analyze the data from LIGYSIS-web database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bebcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'{DATA_PATH}/LIGYSIS-files/LIGYSIS_master_fps_dict.pkl', 'rb') as f:\n",
    "    pkl_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5585d9b",
   "metadata": {},
   "source": [
    "## Filter relevant ligands\n",
    "See CryptoBench study which ligands are considered relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "sys.path.append('/home/vit/Projects/cryptoshow-analysis/src/utils')\n",
    "import cryptoshow_utils\n",
    "\n",
    "IGNORED_GROUPS_LIST = ['HOH', 'DOD', 'WAT', 'UNK', 'ABA', 'MPD', 'GOL', 'SO4', 'PO4']\n",
    "P2RANK_ATOMS_NUM_THRESHOLD = 5\n",
    "\n",
    "# load all ligand smiles\n",
    "ligand_smiles_df = pd.read_csv(f'{DATA_PATH}/ligand.tsv', sep='\\t')\n",
    "cached_smiles = {}\n",
    "\n",
    "\n",
    "def check_ligand_atom_count(smiles):\n",
    "    for i in str(smiles).split(';'):\n",
    "        i = i.strip()\n",
    "        if i in cached_smiles.keys():\n",
    "            if cached_smiles[i]:\n",
    "                return True\n",
    "            else:\n",
    "                continue\n",
    "        try:\n",
    "            molecule = Chem.MolFromSmiles(i)\n",
    "            atoms_count = molecule.GetNumAtoms()\n",
    "        except:\n",
    "            cached_smiles[i] = False\n",
    "            continue\n",
    "\n",
    "        is_valid_smiles = True\n",
    "        if atoms_count < P2RANK_ATOMS_NUM_THRESHOLD:\n",
    "            is_valid_smiles = False\n",
    "\n",
    "        cached_smiles[i] = is_valid_smiles\n",
    "        return is_valid_smiles\n",
    "\n",
    "    return False\n",
    "\n",
    "def is_in_ignored_group(ligand):\n",
    "    return ligand in IGNORED_GROUPS_LIST\n",
    "\n",
    "def is_valid_ligand(ligand):\n",
    "    if ligand in ligand_smiles_df['#CCD'].values:\n",
    "        smiles_versions = ligand_smiles_df.loc[ligand_smiles_df['#CCD'] == ligand, 'SMILES'].values[0].split(';')\n",
    "        for smiles in smiles_versions:\n",
    "            # check if in cached smiles:\n",
    "            if smiles in cached_smiles:\n",
    "                return cached_smiles[smiles]\n",
    "            # check if in ignored groups\n",
    "            if is_in_ignored_group(ligand):\n",
    "                cached_smiles[smiles] = False\n",
    "                return False\n",
    "        # check atom count\n",
    "        return check_ligand_atom_count(smiles_versions)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def initialize_cached_smiles():\n",
    "    # read cached smiles\n",
    "    cached_smiles_path = f'{DATA_PATH}/cached_smiles.csv'\n",
    "    if os.path.exists(cached_smiles_path):\n",
    "        with open(cached_smiles_path, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=';')\n",
    "            for row in reader:\n",
    "                cached_smiles[row[0].strip()] = row[1] == 'True'\n",
    "\n",
    "def filter_ligysis_data(data):\n",
    "    initialize_cached_smiles()\n",
    "    filtered_data = {}\n",
    "    for uniprot_id in data.keys():\n",
    "        for segment_id in data[uniprot_id].keys():\n",
    "            for POI in data[uniprot_id][segment_id].keys():\n",
    "                [_, ligand, _, _] = POI.split('_')\n",
    "                if is_valid_ligand(ligand):\n",
    "                    if uniprot_id not in filtered_data:\n",
    "                        filtered_data[uniprot_id] = {}\n",
    "                    if segment_id not in filtered_data[uniprot_id]:\n",
    "                        filtered_data[uniprot_id][segment_id] = {}\n",
    "                    filtered_data[uniprot_id][segment_id][POI] = data[uniprot_id][segment_id][POI]\n",
    "    return filtered_data\n",
    "\n",
    "ligysis = filter_ligysis_data(pkl_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbad328",
   "metadata": {},
   "source": [
    "## Map binding sites onto the CryptoBench apo structures\n",
    "Take the output of pdb2uniprot tool to map the binding sites from UniProt sequences onto the PDB structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATASET_PATH = f'{DATA_PATH}/cryptobench-dataset/folds/test.json'\n",
    "\n",
    "with open(DATASET_PATH, 'r') as f:\n",
    "    cryptobench_data = json.load(f)\n",
    "\n",
    "cryptobench_data = {key: entry for key, entry in cryptobench_data.items() if '-' not in entry[0]['apo_chain']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf353c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# mapping of UniProt to PDB positions\n",
    "uniprot_to_pdb = {}\n",
    "with open(f'{DATA_PATH}/pdb2uniprot.tsv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    next(reader)  # Skip header \n",
    "    for row in reader:\n",
    "        pdb_id = row[0]\n",
    "        chain_id = row[1]\n",
    "        protein_id = pdb_id + chain_id\n",
    "        uniprot_id = row[4]\n",
    "        # UniProt has to be in LIGYSIS - the LIGYSIS contains UniProts only from the CryptoBench test set\n",
    "        # or, the protein is not in the cryptobench TEST dataset, because the pdb2uniprot.tsv also contains entries from the TRAIN/VALIDATION dataset\n",
    "        assert uniprot_id in ligysis or protein_id not in cryptobench_data, f\"{uniprot_id} not in ligysis and {protein_id} in cryptobench_data\"\n",
    "        if uniprot_id not in uniprot_to_pdb:\n",
    "            uniprot_to_pdb[uniprot_id] = {}\n",
    "        pdb_position = row[3]\n",
    "        if pdb_position == 'null':\n",
    "            continue\n",
    "        uniprot_position = row[6]\n",
    "        uniprot_to_pdb[uniprot_id][uniprot_position] = pdb_position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f241c5",
   "metadata": {},
   "source": [
    "## Merge LIGYSIS and CryptoBench\n",
    "Map LIGYSIS binding sites onto the CryptoBench APO structures if the LIGYSIS binding site residues are present in the APO structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e12f964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5042/5326 valid POIs (94.67%) where all LIGYSIS residues are present in the CryptoBench apo structures (removal of binding sites with missing residues in APO structures)\n"
     ]
    }
   ],
   "source": [
    "number_of_POIs = 0\n",
    "number_of_valid_POIs = 0\n",
    "\n",
    "cryptobench_binding_sites = {}\n",
    "# Add LIGYSIS binding sites to the CryptoBench data if all LIGYSIS residues are present in the CryptoBench apo structure\n",
    "for pdb_id, entries in cryptobench_data.items():\n",
    "    cryptobench_binding_sites[pdb_id] = {}\n",
    "    uniprot_id = entries[0]['uniprot_id']\n",
    "\n",
    "    assert uniprot_id in uniprot_to_pdb, f\"{uniprot_id} not in uniprot_to_pdb\"\n",
    "    # 214 out of 222 structures from the CryptoBench TEST set have a record in LIGYSIS\n",
    "    if uniprot_id in ligysis:\n",
    "        # add LIGYSIS binding sites if all LIGYSIS residues are present in the CryptoBench apo structure\n",
    "        for segment_ids in ligysis[uniprot_id].keys():\n",
    "            for segment_id in segment_ids:\n",
    "                for POI in ligysis[uniprot_id][segment_id].keys():\n",
    "                    # check if the LIGYSIS residues are present in the CryptoBench apo structure\n",
    "                    all_residues_present = True\n",
    "                    # map the LIGYSIS residues from UniProt to PDB positions\n",
    "                    mapped_residues = []\n",
    "                    for residue in ligysis[uniprot_id][segment_id][POI]:\n",
    "                        if str(residue) not in uniprot_to_pdb[uniprot_id]:\n",
    "                            all_residues_present = False\n",
    "                            break\n",
    "                        mapped_residues.append(uniprot_to_pdb[uniprot_id][str(residue)])\n",
    "                    if all_residues_present:\n",
    "                        cryptobench_binding_sites[pdb_id][POI + '_LIGYSIS'] = mapped_residues\n",
    "                        number_of_valid_POIs += 1\n",
    "                    number_of_POIs += 1\n",
    "print(f\"{number_of_valid_POIs}/{number_of_POIs} valid POIs ({number_of_valid_POIs/number_of_POIs*100:.2f}%) where all LIGYSIS residues are present in the CryptoBench apo structures (removal of binding sites with missing residues in APO structures)\")\n",
    "\n",
    "# Add CryptoBench binding sites\n",
    "for pdb_id, entries in cryptobench_data.items():\n",
    "    for entry in entries:\n",
    "        holo_pdb_id = entry['holo_pdb_id']\n",
    "        ligand = entry['ligand']\n",
    "        ligand_index = entry['ligand_index']\n",
    "        ligand_chain_id = entry['ligand_chain']\n",
    "        residues = [i.split('_')[1] for i in entry['apo_pocket_selection']]\n",
    "\n",
    "        POI = '_'.join([holo_pdb_id, ligand, ligand_chain_id, ligand_index, 'CryptoBench'])\n",
    "        if pdb_id not in cryptobench_binding_sites:\n",
    "            cryptobench_binding_sites[pdb_id] = {}\n",
    "        cryptobench_binding_sites[pdb_id][POI] = residues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83a41a",
   "metadata": {},
   "source": [
    "## Cluster binding sites\n",
    "Use LIGYSIS methodology to cluster binding sites together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70a47f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLUSTER_DISTANCE = 0.5 # distance threshold to cut the hierarchical clustering tree, threshold follows the LBS comparison paper: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-024-00923-z)\n",
    "CLUSTERING_METHOD = 'average' # method to use for hierarchical clustering; see https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html\n",
    "\n",
    "# This code was taken from the LIGYSIS repository (https://github.com/bartongroup/LIGYSIS/blob/running-arpeggio/ligysis.py); see LICENSE file\n",
    "\n",
    "def get_intersect_rel_matrix(binding_ress):\n",
    "    \"\"\"\n",
    "    Given a set of ligand binding residues, calculates a\n",
    "    similarity matrix between all the different sets of ligand\n",
    "    binding residues.\n",
    "    \"\"\"\n",
    "    inters = {i: {} for i in range(len(binding_ress))}\n",
    "    for i in range(len(binding_ress)):\n",
    "        inters[i][i] = intersection_rel(binding_ress[i], binding_ress[i])\n",
    "        for j in range(i+1, len(binding_ress)):\n",
    "            inters[i][j] = intersection_rel(binding_ress[i], binding_ress[j])\n",
    "            inters[j][i] = inters[i][j]\n",
    "    return inters\n",
    "\n",
    "def intersection_rel(l1, l2):\n",
    "    \"\"\"\n",
    "    Calculates relative intersection.\n",
    "    \"\"\"\n",
    "    len1 = len(l1)\n",
    "    len2 = len(l2)\n",
    "    I_max = min([len1, len2])\n",
    "    I = len(list(set(l1).intersection(l2)))\n",
    "    return I/I_max\n",
    "\n",
    "def get_binding_site_clusters(binding_sites):\n",
    "    \"\"\"Get binding site clusters.\n",
    "\n",
    "    Args:\n",
    "        binding_sites (list): List of binding sites.\n",
    "    \"\"\"\n",
    "    import scipy\n",
    "    import pandas as pd\n",
    "    \n",
    "    irel_matrix = get_intersect_rel_matrix(binding_sites)\n",
    "    irel_df = pd.DataFrame(irel_matrix)\n",
    "    dist_df = 1 - irel_df # distance matrix in pd.Dataframe() format\n",
    "    condensed_dist_mat = scipy.spatial.distance.squareform(dist_df) # condensed distance matrix to be used for clustering\n",
    "    linkage = scipy.cluster.hierarchy.linkage(condensed_dist_mat, method=CLUSTERING_METHOD, optimal_ordering=True)\n",
    "    return scipy.cluster.hierarchy.cut_tree(linkage, height=CLUSTER_DISTANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2342a4d",
   "metadata": {},
   "source": [
    "### Create annotations\n",
    "Create sequences and map the PDB labeling onto those sequences. Get distance matrix of all the residues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd538ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(f'{DATA_PATH}/cryptobench-clustered-binding-sites.csv', 'w') as out_f:\n",
    "    for pdb_id, binding_sites in cryptobench_binding_sites.items():\n",
    "        POIs = list(binding_sites.keys())\n",
    "        binding_sites = list(binding_sites.values())\n",
    "        # get clusters of binding sites\n",
    "        if len(binding_sites) > 1:\n",
    "            clusters = get_binding_site_clusters(binding_sites).reshape(-1)\n",
    "        else:\n",
    "            clusters = [0]\n",
    "        # loop over each cluster, merge it together, collect the ligands and check if it is cryptic\n",
    "        for cluster_id in range(max(clusters) + 1):\n",
    "            is_cryptic = False\n",
    "            binding_ligands = set()\n",
    "            binding_residues = set()\n",
    "            for i, binding_site in enumerate(binding_sites):\n",
    "                assert len(clusters) == len(POIs), \"Length mismatch\"\n",
    "                if clusters[i] == cluster_id:\n",
    "                    # collect the ligand\n",
    "                    [_, ligand, _, _, source_dataset] = POIs[i].split('_')\n",
    "                    binding_ligands.add(ligand)\n",
    "                    # check if it is cryptic\n",
    "                    if source_dataset == 'CryptoBench':\n",
    "                        is_cryptic = True\n",
    "                    # collect the residues\n",
    "                    binding_residues.update(binding_site)\n",
    "            \n",
    "            chain_id = cryptobench_data[pdb_id][0]['apo_chain']\n",
    "\n",
    "            # retrieve sequence from mmCIF file and map the residues to the mmCIF numbering\n",
    "            binding_residues, sequence = cryptoshow_utils.map_auth_to_mmcif_numbering(pdb_id, chain_id, binding_residues)\n",
    "\n",
    "            # get coordinates\n",
    "            coordinates = cryptoshow_utils.get_coordinates(pdb_id, chain_id)\n",
    "\n",
    "            # write to file\n",
    "            out_f.write(f\"{pdb_id}{cryptobench_data[pdb_id][0]['apo_chain']};{' '.join(binding_ligands)};{'CRYPTIC' if is_cryptic else 'NON_CRYPTIC'};{' '.join(binding_residues)};{sequence}\\n\")\n",
    "            np.save(f'{DATA_PATH}/coordinates/{pdb_id}{chain_id}.npy', coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70ddc4",
   "metadata": {},
   "source": [
    "# Extract CryptoBench train set\n",
    "Extract per-pocket annotations of the CryptoBench train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee04249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "DATASET_PATH = [f'{DATA_PATH}/folds/train-fold-0.json',\n",
    "                f'{DATA_PATH}/folds/train-fold-1.json',\n",
    "                f'{DATA_PATH}/folds/train-fold-2.json',\n",
    "                f'{DATA_PATH}/folds/train-fold-3.json']\n",
    "\n",
    "cryptobench_data = {}\n",
    "for DATASET_PATH_i in DATASET_PATH:\n",
    "    with open(DATASET_PATH_i, 'r') as f:\n",
    "        cryptobench_data_i = json.load(f)\n",
    "    cryptobench_data.update(cryptobench_data_i)\n",
    "\n",
    "cryptobench_data = {key: entry for key, entry in cryptobench_data.items() if '-' not in entry[0]['apo_chain']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35949a6c",
   "metadata": {},
   "source": [
    "### Load mapping \n",
    "Get numbering mapping from sequence to structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc6416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# mapping of UniProt to PDB positions\n",
    "uniprot_to_pdb = {}\n",
    "with open(f'{DATA_PATH}/pdb2uniprot.tsv', newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    next(reader)  # Skip header \n",
    "    for row in reader:\n",
    "        pdb_id = row[0]\n",
    "        chain_id = row[1]\n",
    "        protein_id = pdb_id + chain_id\n",
    "        uniprot_id = row[4]\n",
    "\n",
    "        if uniprot_id not in uniprot_to_pdb:\n",
    "            uniprot_to_pdb[uniprot_id] = {}\n",
    "        pdb_position = row[3]\n",
    "        if pdb_position == 'null':\n",
    "            continue\n",
    "        uniprot_position = row[6]\n",
    "        uniprot_to_pdb[uniprot_id][uniprot_position] = pdb_position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e07186",
   "metadata": {},
   "source": [
    "### Extract from JSON\n",
    "Extract relevant data from the CryptoBench JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea744ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptobench_binding_sites = {}\n",
    "\n",
    "for pdb_id, entries in cryptobench_data.items():\n",
    "    for entry in entries:\n",
    "        holo_pdb_id = entry['holo_pdb_id']\n",
    "        ligand = entry['ligand']\n",
    "        ligand_index = entry['ligand_index']\n",
    "        ligand_chain_id = entry['ligand_chain']\n",
    "        residues = [i.split('_')[1] for i in entry['apo_pocket_selection']]\n",
    "\n",
    "        POI = '_'.join([holo_pdb_id, ligand, ligand_chain_id, ligand_index, 'CryptoBench'])\n",
    "        if pdb_id not in cryptobench_binding_sites:\n",
    "            cryptobench_binding_sites[pdb_id] = {}\n",
    "        cryptobench_binding_sites[pdb_id][POI] = residues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(f'{DATA_PATH}/cryptobench-train-clustered-binding-sites.txt', 'w') as out_f:\n",
    "    for pdb_id, binding_sites in cryptobench_binding_sites.items():\n",
    "        POIs = list(binding_sites.keys())\n",
    "        binding_sites = list(binding_sites.values())\n",
    "        # get clusters of binding sites\n",
    "        if len(binding_sites) > 1:\n",
    "            clusters = get_binding_site_clusters(binding_sites).reshape(-1)\n",
    "        else:\n",
    "            clusters = [0]\n",
    "        # loop over each cluster, merge it together, collect the ligands and check if it is cryptic\n",
    "        for cluster_id in range(max(clusters) + 1):\n",
    "            binding_ligands = set()\n",
    "            binding_residues = set()\n",
    "            for i, binding_site in enumerate(binding_sites):\n",
    "                assert len(clusters) == len(POIs), \"Length mismatch\"\n",
    "                if clusters[i] == cluster_id:\n",
    "                    # collect the ligand\n",
    "                    [_, ligand, _, _, source_dataset] = POIs[i].split('_')\n",
    "                    binding_ligands.add(ligand)\n",
    "                    # collect the residues\n",
    "                    binding_residues.update(binding_site)\n",
    "            \n",
    "            chain_id = cryptobench_data[pdb_id][0]['apo_chain']\n",
    "\n",
    "            # retrieve sequence from mmCIF file and map the residues to the mmCIF numbering\n",
    "            binding_residues, sequence = cryptoshow_utils.map_auth_to_mmcif_numbering(pdb_id, chain_id, binding_residues)\n",
    "\n",
    "            # get coordinates\n",
    "            coordinates = cryptoshow_utils.get_coordinates(pdb_id, chain_id)\n",
    "\n",
    "            # write to file\n",
    "            out_f.write(f\"{pdb_id}{cryptobench_data[pdb_id][0]['apo_chain']};{' '.join(binding_ligands)};'CRYPTIC';{' '.join(binding_residues)};{sequence}\\n\")\n",
    "            np.save(f'{DATA_PATH}/coordinates/{pdb_id}{chain_id}.npy', coordinates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
